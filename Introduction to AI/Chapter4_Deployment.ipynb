{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54bc2a63-0bd3-4f34-9487-6e34da7fb9a4",
   "metadata": {},
   "source": [
    "### Deployment with ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f65c1a-3c0d-451f-ae57-6c520c2493a7",
   "metadata": {},
   "source": [
    "ONNX (Open Neural Network Exchange) is designed to facilitate model deployment across various platforms and frameworks. ONNX provides a standardized format for representing deep learning models, making it possible to export a trained model from one deep learning framework and deploy it in another that supports ONNX.\n",
    "\n",
    "Here are some key aspects of using ONNX for deployment:\n",
    "\n",
    "1. **Interoperability:** ONNX allows you to export models trained in frameworks like PyTorch, TensorFlow, or scikit-learn to a common format. This facilitates interoperability and simplifies the deployment process.\n",
    "\n",
    "2. **Deployment Targets:** ONNX models can be deployed on a wide range of platforms, including edge devices, cloud services, mobile devices, and IoT devices. ONNX Runtime is a cross-platform, high-performance scoring engine for ONNX models that supports deployment in various environments.\n",
    "\n",
    "3. **Ecosystem Support:** Many popular deep learning frameworks and tools support ONNX, making it easier to integrate ONNX models into existing workflows. For example, ONNX models can be used with ONNX Runtime, TensorFlow, OpenVINO, and more.\n",
    "\n",
    "4. **Model Optimization:** ONNX provides tools for optimizing models, including quantization and fusion, which can be important for deployment in resource-constrained environments.\n",
    "\n",
    "5. **Language Support:** ONNX has bindings for several programming languages, making it accessible for deployment with languages like Python, C++, Java, and more.\n",
    "\n",
    "Here is a high-level overview of the steps involved in using ONNX for deployment:\n",
    "\n",
    "- **Export Model:** Export your trained model to ONNX format using the appropriate export functions provided by the deep learning framework (e.g., `torch.onnx.export` in PyTorch).\n",
    "\n",
    "- **Load ONNX Model:** Load the exported ONNX model using an ONNX runtime engine, such as ONNX Runtime.\n",
    "\n",
    "- **Deploy Model:** Deploy the ONNX model on the target platform or framework. This could involve integrating the model into a web service, deploying it on an edge device, or incorporating it into a mobile application.\n",
    "\n",
    "- **Inference:** Use the deployed ONNX model to perform inference on new data.\n",
    "\n",
    "By using ONNX, you can achieve greater flexibility in deploying your models across different environments and frameworks, making it a valuable tool for production deployment.\n",
    "\n",
    "Here's an example of how you can export your PyTorch model to ONNX:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0bdf4aa-aaa9-44f6-8609-166748786086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(32, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc1.weight : Float(128, 9216, strides=[9216, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(128, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(10, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/conv1/Conv_output_0 : Float(1, 32, 26, 26, strides=[21632, 676, 26, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv1/Conv\"](%input.1, %conv1.weight, %conv1.bias), scope: mnist_model.Net::/torch.nn.modules.conv.Conv2d::conv1 # /home/anil/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/Relu_output_0 : Float(1, 32, 26, 26, strides=[21632, 676, 26, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu\"](%/conv1/Conv_output_0), scope: mnist_model.Net:: # /home/anil/.local/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/conv2/Conv_output_0 : Float(1, 64, 24, 24, strides=[36864, 576, 24, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv2/Conv\"](%/Relu_output_0, %conv2.weight, %conv2.bias), scope: mnist_model.Net::/torch.nn.modules.conv.Conv2d::conv2 # /home/anil/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/Relu_1_output_0 : Float(1, 64, 24, 24, strides=[36864, 576, 24, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu_1\"](%/conv2/Conv_output_0), scope: mnist_model.Net:: # /home/anil/.local/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/MaxPool_output_0 : Float(1, 64, 12, 12, strides=[9216, 144, 12, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/MaxPool\"](%/Relu_1_output_0), scope: mnist_model.Net:: # /home/anil/.local/lib/python3.10/site-packages/torch/nn/functional.py:791:0\n",
      "  %/Flatten_output_0 : Float(1, 9216, strides=[9216, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/Flatten\"](%/MaxPool_output_0), scope: mnist_model.Net:: # /home/anil/HER-workshop/Introduction to AI/mnist_model.py:22:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Flatten_output_0, %fc1.weight, %fc1.bias), scope: mnist_model.Net::/torch.nn.modules.linear.Linear::fc1 # /home/anil/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_2_output_0 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu_2\"](%/fc1/Gemm_output_0), scope: mnist_model.Net:: # /home/anil/.local/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Relu_2_output_0, %fc2.weight, %fc2.bias), scope: mnist_model.Net::/torch.nn.modules.linear.Linear::fc2 # /home/anil/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %18 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::LogSoftmax[axis=1, onnx_name=\"/LogSoftmax\"](%/fc2/Gemm_output_0), scope: mnist_model.Net:: # /home/anil/.local/lib/python3.10/site-packages/torch/nn/functional.py:1945:0\n",
      "  return (%18)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mnist_model import Net\n",
    "\n",
    "def main():\n",
    "    pytorch_model = Net()\n",
    "\n",
    "    # Load only the model weights and biases\n",
    "    state_dict = torch.load('mnist_cnn.pt')['model_state_dict']\n",
    "    \n",
    "    # Update the model state dictionary to match the key names\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}  # Handle if model was saved with DataParallel\n",
    "\n",
    "    # Load the updated state dictionary\n",
    "    pytorch_model.load_state_dict(state_dict)\n",
    "    \n",
    "    pytorch_model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(pytorch_model, dummy_input, 'onnx_model.onnx', verbose=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efb920-4c3c-4249-b915-ddcd0a4404d7",
   "metadata": {},
   "source": [
    "### Excersie\n",
    "- Try to run the model pipeline on your own dataset and export them into ONNX.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe53653-ba01-4e0d-aea0-fd2ead3bd991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
